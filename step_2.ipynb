{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries (install in your environment first)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log2, sqrt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import os\n",
    "# Setting up the logger\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filePath = 'QRNGvsPRNG_TrainingData.txt'\n",
    "df = pd.read_csv(data_filePath, sep=' ',header=None, dtype={\"data\": str, \"label\": np.int64})\n",
    "df.columns = [\"data\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_rows_with_same_labels(df, nrows):\n",
    "    df['label'] = df['label'].apply(lambda x: x-1) # Change the labels to 0 and 1\n",
    "    qrng_df = df[df['label'] == 0]\n",
    "    prng_df = df[df['label'] == 1]\n",
    "    \n",
    "    qrng_data = qrng_df['data'].values\n",
    "    prng_data = prng_df['data'].values\n",
    "    \n",
    "    qrng_data =[ ''.join([qrng_data[i+j] for j in range(nrows)]) for i in range(0, len(qrng_data), nrows)]\n",
    "    prng_data = [''.join([prng_data[i+j] for j in range(nrows)]) for i in range(0, len(prng_data), nrows)]\n",
    "    \n",
    "    qrng_df = pd.DataFrame(qrng_data, columns=['data'])\n",
    "    prng_df = pd.DataFrame(prng_data, columns=['data'])\n",
    "    \n",
    "    qrng_df['label'] = 0\n",
    "    prng_df['label'] = 1\n",
    "\n",
    "    combined_df = pd.concat([qrng_df, prng_df], ignore_index=True)\n",
    "    combined_df[\"length\"] = combined_df[\"data\"].apply(lambda x: len(x))\n",
    "    combined_df[\"length\"].value_counts()\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Labels and Train_Test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data  label  length\n",
      "0  0000101000100111111110011011110111101101010111...      0     500\n",
      "1  1110110111100100011101001000011110000111111101...      0     500\n",
      "2  1101000010011000100101110010111101010011011000...      0     500\n",
      "3  1001011011100100100101110111101111000011101001...      0     500\n",
      "4  1011000011010000101000100010110000011101010101...      0     500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(((3456, 500), (3456, 1)), ((384, 500), (384, 1)), ((960, 500), (960, 1)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = merge_rows_with_same_labels(df, 5)\n",
    "print(df.head())\n",
    "X = df['data'].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Split the each string of X into a list of characters, and convert them to integers\n",
    "X = np.array([list(map(int, list(x))) for x in X])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, shuffle=True, random_state=42)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1]),\n",
       " array([0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1], y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNClassifier100(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNClassifier100, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(100, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 64)   \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "    \n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = F.relu(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        # return x\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class RNClassifier200(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNClassifier200, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(200, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 64)   \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "    \n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = F.relu(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        # return x\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "class RNClassifier400(nn.Module):\n",
    "    lr = 0.0001\n",
    "    batch_size = 16\n",
    "    def __init__(self):\n",
    "        super(RNClassifier400, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(400, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 64)   \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "    \n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = F.relu(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        # return x\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class RNClassifier800(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNClassifier800, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(800, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 64)   \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "    \n",
    "        self.fc5 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = F.relu(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        # return x\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class RNClassifier500(nn.Module):\n",
    "    lr = 0.0001\n",
    "    batch_size = 16\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RNClassifier500, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(500, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 64)   \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "    \n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = F.relu(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        # return x\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# model = RNClassifier500()\n",
    "# Split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yubraj/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "INFO:root:RNClassifier500\n",
      "INFO:root:Epoch: 0, Loss: 0.7256984307810113\n",
      "INFO:root:Epoch: 0, Validation Loss:0.7017377391457558, Accuracy: 0.4973958333333333, accurate: 191, total: 384\n",
      "INFO:root:Epoch: 1, Loss: 0.7125358324911859\n",
      "INFO:root:Epoch: 2, Loss: 0.7088457707453657\n",
      "INFO:root:Epoch: 3, Loss: 0.7082038783916721\n",
      "INFO:root:Epoch: 4, Loss: 0.6999357549680604\n",
      "INFO:root:Epoch: 5, Loss: 0.6857246797945764\n",
      "INFO:root:Epoch: 5, Validation Loss:0.7022880986332893, Accuracy: 0.5104166666666666, accurate: 196, total: 384\n",
      "INFO:root:Epoch: 6, Loss: 0.6872103252896556\n",
      "INFO:root:Epoch: 7, Loss: 0.6821459918110458\n",
      "INFO:root:Epoch: 8, Loss: 0.676004535622067\n",
      "INFO:root:Epoch: 9, Loss: 0.6778175303781474\n",
      "INFO:root:Epoch: 10, Loss: 0.6719838560179427\n",
      "INFO:root:Epoch: 10, Validation Loss:0.7074536979198456, Accuracy: 0.4739583333333333, accurate: 182, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 1 epochs.\n",
      "INFO:root:Epoch: 11, Loss: 0.657430165067867\n",
      "INFO:root:Epoch: 12, Loss: 0.6521387641076688\n",
      "INFO:root:Epoch: 13, Loss: 0.6450164013162807\n",
      "INFO:root:Epoch: 14, Loss: 0.6414519937502013\n",
      "INFO:root:Epoch: 15, Loss: 0.6355107320403611\n",
      "INFO:root:Epoch: 15, Validation Loss:0.7214628433187803, Accuracy: 0.484375, accurate: 186, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 2 epochs.\n",
      "INFO:root:Epoch: 16, Loss: 0.6271162424926404\n",
      "INFO:root:Epoch: 17, Loss: 0.6221267019433004\n",
      "INFO:root:Epoch: 18, Loss: 0.6048124693334103\n",
      "INFO:root:Epoch: 19, Loss: 0.6033939782668043\n",
      "INFO:root:Epoch: 20, Loss: 0.5995577468916222\n",
      "INFO:root:Epoch: 20, Validation Loss:0.7449865018328031, Accuracy: 0.4895833333333333, accurate: 188, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 3 epochs.\n",
      "INFO:root:Epoch: 21, Loss: 0.5914192045176471\n",
      "INFO:root:Epoch: 22, Loss: 0.5789410023501625\n",
      "INFO:root:Epoch: 23, Loss: 0.5704680668811003\n",
      "INFO:root:Epoch: 24, Loss: 0.5782838535529596\n",
      "INFO:root:Epoch: 25, Loss: 0.5633534595922187\n",
      "INFO:root:Epoch: 25, Validation Loss:0.7677384441097578, Accuracy: 0.5104166666666666, accurate: 196, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 4 epochs.\n",
      "INFO:root:Epoch: 26, Loss: 0.5598711975746684\n",
      "INFO:root:Epoch: 27, Loss: 0.549019102283098\n",
      "INFO:root:Epoch: 28, Loss: 0.5480006927693332\n",
      "INFO:root:Epoch: 29, Loss: 0.5502619746106642\n",
      "INFO:root:Epoch: 30, Loss: 0.5308191730744309\n",
      "INFO:root:Epoch: 30, Validation Loss:0.7968816931049029, Accuracy: 0.5052083333333334, accurate: 194, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 5 epochs.\n",
      "INFO:root:Epoch: 31, Loss: 0.5212667491976861\n",
      "INFO:root:Epoch: 32, Loss: 0.5187000646083443\n",
      "INFO:root:Epoch: 33, Loss: 0.5024769598687137\n",
      "INFO:root:Epoch: 34, Loss: 0.502177014257069\n",
      "INFO:root:Epoch: 35, Loss: 0.5041530660733029\n",
      "INFO:root:Epoch: 35, Validation Loss:0.8180388410886129, Accuracy: 0.5026041666666666, accurate: 193, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 6 epochs.\n",
      "INFO:root:Epoch: 36, Loss: 0.48543872456583714\n",
      "INFO:root:Epoch: 37, Loss: 0.48262517561239227\n",
      "INFO:root:Epoch: 38, Loss: 0.4845534940422685\n",
      "INFO:root:Epoch: 39, Loss: 0.4744307793400906\n",
      "INFO:root:Epoch: 40, Loss: 0.4855993849535783\n",
      "INFO:root:Epoch: 40, Validation Loss:0.8445181325078011, Accuracy: 0.5104166666666666, accurate: 196, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 7 epochs.\n",
      "INFO:root:Epoch: 41, Loss: 0.4721996751786382\n",
      "INFO:root:Epoch: 42, Loss: 0.4633437186755516\n",
      "INFO:root:Epoch: 43, Loss: 0.4492918352286021\n",
      "INFO:root:Epoch: 44, Loss: 0.4468338232211493\n",
      "INFO:root:Epoch: 45, Loss: 0.4372847723878092\n",
      "INFO:root:Epoch: 45, Validation Loss:0.8827584907412529, Accuracy: 0.5026041666666666, accurate: 193, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 8 epochs.\n",
      "INFO:root:Epoch: 46, Loss: 0.4357193560787925\n",
      "INFO:root:Epoch: 47, Loss: 0.43282384735842544\n",
      "INFO:root:Epoch: 48, Loss: 0.4286685688766064\n",
      "INFO:root:Epoch: 49, Loss: 0.41331226268300303\n",
      "INFO:root:Epoch: 50, Loss: 0.4177920419584822\n",
      "INFO:root:Epoch: 50, Validation Loss:0.8871399462223053, Accuracy: 0.515625, accurate: 198, total: 384\n",
      "INFO:root:Epoch: 51, Loss: 0.4046052192096357\n",
      "INFO:root:Epoch: 52, Loss: 0.3839407044428366\n",
      "INFO:root:Epoch: 53, Loss: 0.4038236129477068\n",
      "INFO:root:Epoch: 54, Loss: 0.39288071135955827\n",
      "INFO:root:Epoch: 55, Loss: 0.394966753475644\n",
      "INFO:root:Epoch: 55, Validation Loss:0.9198255042235056, Accuracy: 0.5026041666666666, accurate: 193, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 1 epochs.\n",
      "INFO:root:Epoch: 56, Loss: 0.3769778113260313\n",
      "INFO:root:Epoch: 57, Loss: 0.375549642438138\n",
      "INFO:root:Epoch: 58, Loss: 0.3674379591312673\n",
      "INFO:root:Epoch: 59, Loss: 0.36621814041777895\n",
      "INFO:root:Epoch: 60, Loss: 0.3551423169189581\n",
      "INFO:root:Epoch: 60, Validation Loss:0.9583033795158068, Accuracy: 0.5, accurate: 192, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 2 epochs.\n",
      "INFO:root:Epoch: 61, Loss: 0.3707755412561474\n",
      "INFO:root:Epoch: 62, Loss: 0.3580550468285327\n",
      "INFO:root:Epoch: 63, Loss: 0.35469918615288204\n",
      "INFO:root:Epoch: 64, Loss: 0.3506654990767991\n",
      "INFO:root:Epoch: 65, Loss: 0.35109171492082103\n",
      "INFO:root:Epoch: 65, Validation Loss:0.9863052616516749, Accuracy: 0.5078125, accurate: 195, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 3 epochs.\n",
      "INFO:root:Epoch: 66, Loss: 0.3451913569674448\n",
      "INFO:root:Epoch: 67, Loss: 0.33660651546799475\n",
      "INFO:root:Epoch: 68, Loss: 0.32717154423395794\n",
      "INFO:root:Epoch: 69, Loss: 0.33155737120520185\n",
      "INFO:root:Epoch: 70, Loss: 0.32623562434067327\n",
      "INFO:root:Epoch: 70, Validation Loss:1.0493708302577336, Accuracy: 0.4921875, accurate: 189, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 4 epochs.\n",
      "INFO:root:Epoch: 71, Loss: 0.32649990312617133\n",
      "INFO:root:Epoch: 72, Loss: 0.30848194958849084\n",
      "INFO:root:Epoch: 73, Loss: 0.3413081180166315\n",
      "INFO:root:Epoch: 74, Loss: 0.3174469376120854\n",
      "INFO:root:Epoch: 75, Loss: 0.3057695169484726\n",
      "INFO:root:Epoch: 75, Validation Loss:1.0512721414367359, Accuracy: 0.5, accurate: 192, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 5 epochs.\n",
      "INFO:root:Epoch: 76, Loss: 0.3341767070814967\n",
      "INFO:root:Epoch: 77, Loss: 0.3037622718623391\n",
      "INFO:root:Epoch: 78, Loss: 0.2995946201392346\n",
      "INFO:root:Epoch: 79, Loss: 0.31265884837894525\n",
      "INFO:root:Epoch: 80, Loss: 0.28772338138272363\n",
      "INFO:root:Epoch: 80, Validation Loss:1.0417241156101227, Accuracy: 0.5078125, accurate: 195, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 6 epochs.\n",
      "INFO:root:Epoch: 81, Loss: 0.3028268355324312\n",
      "INFO:root:Epoch: 82, Loss: 0.29945608387114825\n",
      "INFO:root:Epoch: 83, Loss: 0.29424420595858936\n",
      "INFO:root:Epoch: 84, Loss: 0.2950873365798206\n",
      "INFO:root:Epoch: 85, Loss: 0.2734618210061281\n",
      "INFO:root:Epoch: 85, Validation Loss:1.083426925043265, Accuracy: 0.5026041666666666, accurate: 193, total: 384\n",
      "INFO:root:Validation accuracy did not improve for 7 epochs.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m                 logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarly stopping triggered after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatience\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs with no improvement. Best accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_accu_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRNClassifier500\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 43\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, n_bits, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m     41\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     42\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 43\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     47\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:469\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    468\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 469\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# call optimizer step pre hooks\u001b[39;49;00m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_global_optimizer_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, n_bits,  X_train, y_train, X_val, y_val):\n",
    "    os.makedirs(f'{n_bits}bit_model', exist_ok=True)\n",
    "    \n",
    "    # logging file\n",
    "    logging.basicConfig(filename=f'{n_bits}bit_model/{model.__class__.__name__}.log', level=logging.INFO)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=model.lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=100, verbose=True, factor=0.5)\n",
    "\n",
    "    epochs = 1000\n",
    "    bactch_size = model.batch_size\n",
    "    patience = 50\n",
    "\n",
    "    X_train = torch.tensor(X_train).float()\n",
    "    y_train = torch.tensor(y_train).float()\n",
    "    X_val = torch.tensor(X_val).float()\n",
    "    y_val = torch.tensor(y_val).float()\n",
    "\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_data, batch_size=bactch_size, shuffle=True)\n",
    "\n",
    "    val_data = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_data, batch_size=bactch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    best_val_accuracy = 0\n",
    "    best_accu_epoch = 0\n",
    "    \n",
    "    logging.info(model.__class__.__name__)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        logging.info(f'Epoch: {epoch}, Loss: {avg_train_loss}')\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            model.eval()\n",
    "            total_loss = 0.0\n",
    "            accurate = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader, 0):\n",
    "                    inputs, labels = data\n",
    "                    outputs = model(inputs)\n",
    "                    # print(outputs)\n",
    "                    # output = torch.argmax(outputs, 1)\n",
    "                    output = torch.round(outputs)\n",
    "                    accurate += torch.sum(output == labels).item()\n",
    "                    \n",
    "                    val_loss = criterion(outputs, labels)\n",
    "                    total_loss += val_loss.item()\n",
    "            \n",
    "            avg_val_loss = total_loss / len(val_loader)\n",
    "            accuracy = accurate / len(y_val)\n",
    "            \n",
    "            # scheduler.step(avg_val_loss)\n",
    "            \n",
    "            logging.info(f'Epoch: {epoch}, Validation Loss:{avg_val_loss}, Accuracy: {accuracy}, accurate: {accurate}, total: {len(y_val)}')\n",
    "            \n",
    "            # Check if validation loss improved\n",
    "            if accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = accuracy\n",
    "                patience_counter = 0  # Reset counter if validation loss improved\n",
    "                best_accu_epoch = epoch\n",
    "                torch.save(model.state_dict(), f'model/best_model_epoch{epoch}.pth')  # Save the best model\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                logging.info(f\"Validation accuracy did not improve for {patience_counter} epochs.\")\n",
    "            \n",
    "            #  # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                logging.info(f\"Early stopping triggered after {patience} epochs with no improvement. Best accuracy: {best_val_accuracy} at epoch {best_accu_epoch}\")\n",
    "                break\n",
    "\n",
    "train(RNClassifier500(), 500, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNClassifier500\n",
      "(960, 500) (960, 1)\n",
      "Test Loss: 0.7012461584061385, Accuracy: 0.503125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/1546446104.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "#### BEST EPOCH 125 for 500 bits input\n",
    "# Calculate the accuracy of the model\n",
    "best_epoch = 125\n",
    "model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n",
    "model.eval()\n",
    "test_data = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())\n",
    "test_loader = DataLoader(test_data, batch_size=bactch_size, shuffle=True)\n",
    "print(model.__class__.__name__)\n",
    "print(X_test.shape, y_test.shape)\n",
    "total_loss = 0.0\n",
    "accurate = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        output = torch.round(outputs)\n",
    "        accurate += torch.sum(output == labels).item()\n",
    "        \n",
    "        test_loss = criterion(outputs, labels)\n",
    "        total_loss += test_loss.item()\n",
    "\n",
    "avg_test_loss = total_loss / len(test_loader)\n",
    "accuracy = accurate / len(y_test)\n",
    "print(f'Test Loss: {avg_test_loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNClassifier800\n",
      "(600, 800) (600, 1)\n",
      "Test Loss: 0.6935586561759313, Accuracy: 0.5116666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/3978821641.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "#### BEST EPOCH 105 for 800 bits input\n",
    "# Calculate the accuracy of the model\n",
    "best_epoch = 105\n",
    "model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n",
    "model.eval()\n",
    "test_data = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())\n",
    "test_loader = DataLoader(test_data, batch_size=bactch_size, shuffle=True)\n",
    "print(model.__class__.__name__)\n",
    "print(X_test.shape, y_test.shape)\n",
    "total_loss = 0.0\n",
    "accurate = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        output = torch.round(outputs)\n",
    "        accurate += torch.sum(output == labels).item()\n",
    "        \n",
    "        test_loss = criterion(outputs, labels)\n",
    "        total_loss += test_loss.item()\n",
    "\n",
    "avg_test_loss = total_loss / len(test_loader)\n",
    "accuracy = accurate / len(y_test)\n",
    "print(f'Test Loss: {avg_test_loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNClassifier400\n",
      "(1200, 400) (1200, 1)\n",
      "Test Loss: 0.691421952744325, Accuracy: 0.5316666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/2366530904.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "#### BEST EPOCH 145 for 400 bits input\n",
    "# Calculate the accuracy of the model\n",
    "best_epoch = 145\n",
    "model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n",
    "model.eval()\n",
    "test_data = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())\n",
    "test_loader = DataLoader(test_data, batch_size=bactch_size, shuffle=True)\n",
    "print(model.__class__.__name__)\n",
    "print(X_test.shape, y_test.shape)\n",
    "total_loss = 0.0\n",
    "accurate = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        output = torch.round(outputs)\n",
    "        accurate += torch.sum(output == labels).item()\n",
    "        \n",
    "        test_loss = criterion(outputs, labels)\n",
    "        total_loss += test_loss.item()\n",
    "\n",
    "avg_test_loss = total_loss / len(test_loader)\n",
    "accuracy = accurate / len(y_test)\n",
    "print(f'Test Loss: {avg_test_loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNClassifier200\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8x200 and 100x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[197], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[195], line 53\u001b[0m, in \u001b[0;36mRNClassifier200.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 53\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x200 and 100x64)"
     ]
    }
   ],
   "source": [
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "best_val_accuracy = 0\n",
    "best_accu_epoch = 0\n",
    "print(model.__class__.__name__)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    print(f'Epoch: {epoch}, Loss: {avg_train_loss}')\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        accurate = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader, 0):\n",
    "                inputs, labels = data\n",
    "                outputs = model(inputs)\n",
    "                # print(outputs)\n",
    "                # output = torch.argmax(outputs, 1)\n",
    "                output = torch.round(outputs)\n",
    "                accurate += torch.sum(output == labels).item()\n",
    "                \n",
    "                val_loss = criterion(outputs, labels)\n",
    "                total_loss += val_loss.item()\n",
    "        \n",
    "        avg_val_loss = total_loss / len(val_loader)\n",
    "        accuracy = accurate / len(y_val)\n",
    "        \n",
    "        # scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch: {epoch}, Validation Loss:{avg_val_loss}, Accuracy: {accuracy}, accurate: {accurate}, total: {len(y_val)}')\n",
    "        \n",
    "        # Check if validation loss improved\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            patience_counter = 0  # Reset counter if validation loss improved\n",
    "            best_accu_epoch = epoch\n",
    "            torch.save(model.state_dict(), f'model/best_model_epoch{epoch}.pth')  # Save the best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation accuracy did not improve for {patience_counter} epochs.\")\n",
    "        \n",
    "        #  # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs with no improvement. Best accuracy: {best_val_accuracy} at epoch {best_accu_epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6979442413647969, Accuracy: 0.5225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/719008882.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### BEST EPOCH 60 for 100 bits input\n",
    "# Calculate the accuracy of the model\n",
    "\n",
    "def test_model(model, path, X_test, y_test):\n",
    "    criterion = nn.BCELoss()\n",
    "    model.load_state_dict(torch.load(f'{path}'))\n",
    "    # model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n",
    "    model.eval()\n",
    "    test_data = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())\n",
    "    test_loader = DataLoader(test_data, batch_size=model.batch_size, shuffle=True)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    accurate = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            output = torch.round(outputs)\n",
    "            accurate += torch.sum(output == labels).item()\n",
    "            \n",
    "            test_loss = criterion(outputs, labels)\n",
    "            total_loss += test_loss.item()\n",
    "\n",
    "    avg_test_loss = total_loss / len(test_loader)\n",
    "    accuracy = accurate / len(y_test)\n",
    "    print(f'Test Loss: {avg_test_loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wigglerng-jNs1t9jR-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
