{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries (install in your environment first)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log2, sqrt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filePath = 'QRNGvsPRNG_TrainingData.txt'\n",
    "df = pd.read_csv(data_filePath, sep=' ',header=None, dtype={\"data\": str, \"label\": np.int64})\n",
    "df.columns = [\"data\", \"label\"]\n",
    "\n",
    "df['label'] = df['label'].apply(lambda x: x-1) # Change the labels to 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00001010001001111111100110111101111011010101111100110110011011101111111101111100010110011100111101010100101111010000110010000101001110101001001010111011110010010011100011100001110100111111101000010001\n",
      "(12000,)\n",
      "(12000,)\n",
      "00001010001001111111100110111101111011010101111100110110011011101111111101111100010110011100111101010100101111010000110010000101001110101001001010111011110010010011100011100001110100111111101000010001100010101010010001110010111101111100111001110110010000001011111111111001010111101110100100011101010101111011000101100100000111111110011100011001100100101001101001011001100100110011111001110011001101101111100000011110111111111111101001100100011010011110111110110011111101000100100111011001001000001010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "length\n",
       "500    4800\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrng_df = df[df['label'] == 0]\n",
    "prng_df = df[df['label'] == 1]\n",
    "\n",
    "# concat every 2 rows to get 1 row\n",
    "qrng_data = qrng_df['data'].values\n",
    "prng_data = prng_df['data'].values\n",
    "\n",
    "print(qrng_data[0] + qrng_data[1])\n",
    "print(qrng_data.shape), print(prng_data.shape)\n",
    "\n",
    "# qrng_data =[(qrng_data[i] + qrng_data[i+1]) for i in range(0, len(qrng_data), 2)]\n",
    "# prng_data = [(prng_data[i] + prng_data[i+1]) for i in range(0, len(prng_data), 2)]\n",
    "\n",
    "# qrng_data =[(qrng_data[i] + qrng_data[i+1]) for i in range(0, len(qrng_data), 2)]\n",
    "# prng_data = [(prng_data[i] + prng_data[i+1]) for i in range(0, len(prng_data), 2)]\n",
    "\n",
    "# qrng_data =[(qrng_data[i] + qrng_data[i+1]) for i in range(0, len(qrng_data), 2)]\n",
    "# prng_data = [(prng_data[i] + prng_data[i+1]) for i in range(0, len(prng_data), 2)]\n",
    "\n",
    "# concat every 5 rows to get 1 row\n",
    "qrng_data =[(qrng_data[i] + qrng_data[i+1] + qrng_data[i+2] + qrng_data[i+3] + qrng_data[i+4]) for i in range(0, len(qrng_data), 5)]\n",
    "prng_data = [(prng_data[i] + prng_data[i+1] + prng_data[i+2] + prng_data[i+3] + prng_data[i+4]) for i in range(0, len(prng_data), 5)]\n",
    "\n",
    "print(qrng_data[0])\n",
    "qrng_df = pd.DataFrame(qrng_data, columns=['data'])\n",
    "prng_df = pd.DataFrame(prng_data, columns=['data'])\n",
    "qrng_df['label'] = 0\n",
    "prng_df['label'] = 1\n",
    "\n",
    "combined_df = pd.concat([qrng_df, prng_df], ignore_index=True)\n",
    "combined_df[\"length\"] = combined_df[\"data\"].apply(lambda x: len(x))\n",
    "combined_df[\"length\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Labels and Train_Test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = combined_df\n",
    "X = df['data'].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Split the each string of X into a list of characters, and convert them to integers\n",
    "X = np.array([list(map(int, list(x))) for x in X])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, shuffle=True, random_state=42)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((3456, 500), (3456, 1)), ((384, 500), (384, 1)), ((960, 500), (960, 1)))"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1]),\n",
       " array([0]))"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1], y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNClassifier100(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNClassifie100, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(100, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 64)   \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "    \n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = F.relu(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        # return x\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class RNClassifier200(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNClassifier200, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(200, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 64)   \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "    \n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = F.relu(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        # return x\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "class RNClassifier400(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNClassifier400, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(400, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 64)   \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "    \n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = F.relu(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        # return x\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class RNClassifier800(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNClassifier800, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(800, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 64)   \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "    \n",
    "        self.fc5 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = F.relu(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        # return x\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class RNClassifier500(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNClassifier500, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(500, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 64)   \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "    \n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = F.relu(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        # return x\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "    \n",
    "model = RNClassifier500()\n",
    "# Split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/1080680609.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train).float()\n",
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/1080680609.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train).float()\n",
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/1080680609.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val).float()\n",
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/1080680609.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val = torch.tensor(y_val).float()\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=100, verbose=True, factor=0.5)\n",
    "\n",
    "epochs = 10000\n",
    "bactch_size = 2\n",
    "patience =50\n",
    "\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_val = torch.tensor(X_val).float()\n",
    "y_val = torch.tensor(y_val).float()\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=bactch_size, shuffle=True)\n",
    "\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_data, batch_size=bactch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNClassifier500\n",
      "Epoch: 0, Loss: 0.7232027693513643\n",
      "Epoch: 0, Validation Loss:0.6935406328799824, Accuracy: 0.5104166666666666, accurate: 196, total: 384\n",
      "Epoch: 1, Loss: 0.7321509498654416\n",
      "Epoch: 2, Loss: 0.7251305482005356\n",
      "Epoch: 3, Loss: 0.7196984538911946\n",
      "Epoch: 4, Loss: 0.7264776239689026\n",
      "Epoch: 5, Loss: 0.7260808506635604\n",
      "Epoch: 5, Validation Loss:0.6957484991289675, Accuracy: 0.5182291666666666, accurate: 199, total: 384\n",
      "Epoch: 6, Loss: 0.7274316358724954\n",
      "Epoch: 7, Loss: 0.7269406419040428\n",
      "Epoch: 8, Loss: 0.7237786960063709\n",
      "Epoch: 9, Loss: 0.7315391473196171\n",
      "Epoch: 10, Loss: 0.7224674905581331\n",
      "Epoch: 10, Validation Loss:0.6946318730091056, Accuracy: 0.5104166666666666, accurate: 196, total: 384\n",
      "Validation accuracy did not improve for 1 epochs.\n",
      "Epoch: 11, Loss: 0.7261323729116056\n",
      "Epoch: 12, Loss: 0.7281591004381577\n",
      "Epoch: 13, Loss: 0.729335764742077\n",
      "Epoch: 14, Loss: 0.7229801381041331\n",
      "Epoch: 15, Loss: 0.7255748202938035\n",
      "Epoch: 15, Validation Loss:0.6910719083001217, Accuracy: 0.5364583333333334, accurate: 206, total: 384\n",
      "Epoch: 16, Loss: 0.7300191843123348\n",
      "Epoch: 17, Loss: 0.7220448618064876\n",
      "Epoch: 18, Loss: 0.720130428361396\n",
      "Epoch: 19, Loss: 0.7273149192333221\n",
      "Epoch: 20, Loss: 0.7281606283418283\n",
      "Epoch: 20, Validation Loss:0.6962867174297571, Accuracy: 0.4973958333333333, accurate: 191, total: 384\n",
      "Validation accuracy did not improve for 1 epochs.\n",
      "Epoch: 21, Loss: 0.7256941773445794\n",
      "Epoch: 22, Loss: 0.7302040727636604\n",
      "Epoch: 23, Loss: 0.7254460311529262\n",
      "Epoch: 24, Loss: 0.7256707015740512\n",
      "Epoch: 25, Loss: 0.7350665291736799\n",
      "Epoch: 25, Validation Loss:0.6949610986436406, Accuracy: 0.5182291666666666, accurate: 199, total: 384\n",
      "Validation accuracy did not improve for 2 epochs.\n",
      "Epoch: 26, Loss: 0.7201682823986091\n",
      "Epoch: 27, Loss: 0.722856387409761\n",
      "Epoch: 28, Loss: 0.7291044559571203\n",
      "Epoch: 29, Loss: 0.7271549072013133\n",
      "Epoch: 30, Loss: 0.728365668207752\n",
      "Epoch: 30, Validation Loss:0.6953856156518062, Accuracy: 0.5078125, accurate: 195, total: 384\n",
      "Validation accuracy did not improve for 3 epochs.\n",
      "Epoch: 31, Loss: 0.7238061258534866\n",
      "Epoch: 32, Loss: 0.7220092234374197\n",
      "Epoch: 33, Loss: 0.7276091827597055\n",
      "Epoch: 34, Loss: 0.7316631875345828\n",
      "Epoch: 35, Loss: 0.7244827172339514\n",
      "Epoch: 35, Validation Loss:0.6918417488535246, Accuracy: 0.515625, accurate: 198, total: 384\n",
      "Validation accuracy did not improve for 4 epochs.\n",
      "Epoch: 36, Loss: 0.7271152917916576\n",
      "Epoch: 37, Loss: 0.7313441601926806\n",
      "Epoch: 38, Loss: 0.7277698521329848\n",
      "Epoch: 39, Loss: 0.7371637856035873\n",
      "Epoch: 40, Loss: 0.7326712348170716\n",
      "Epoch: 40, Validation Loss:0.69489493748794, Accuracy: 0.5182291666666666, accurate: 199, total: 384\n",
      "Validation accuracy did not improve for 5 epochs.\n",
      "Epoch: 41, Loss: 0.7312955508888181\n",
      "Epoch: 42, Loss: 0.7290263357572258\n",
      "Epoch: 43, Loss: 0.7255054180234395\n",
      "Epoch: 44, Loss: 0.7313324438614978\n",
      "Epoch: 45, Loss: 0.7276491533050796\n",
      "Epoch: 45, Validation Loss:0.6941210101358593, Accuracy: 0.5026041666666666, accurate: 193, total: 384\n",
      "Validation accuracy did not improve for 6 epochs.\n",
      "Epoch: 46, Loss: 0.7314712670804174\n",
      "Epoch: 47, Loss: 0.7267853682688264\n",
      "Epoch: 48, Loss: 0.7255109630835553\n",
      "Epoch: 49, Loss: 0.7322233598593071\n",
      "Epoch: 50, Loss: 0.7304162214061728\n",
      "Epoch: 50, Validation Loss:0.6932049376579622, Accuracy: 0.5260416666666666, accurate: 202, total: 384\n",
      "Validation accuracy did not improve for 7 epochs.\n",
      "Epoch: 51, Loss: 0.7240560467320459\n",
      "Epoch: 52, Loss: 0.7261169430868769\n",
      "Epoch: 53, Loss: 0.7297380604712224\n",
      "Epoch: 54, Loss: 0.7233351190936648\n",
      "Epoch: 55, Loss: 0.7210555865550069\n",
      "Epoch: 55, Validation Loss:0.694670936713616, Accuracy: 0.5052083333333334, accurate: 194, total: 384\n",
      "Validation accuracy did not improve for 8 epochs.\n",
      "Epoch: 56, Loss: 0.7316150808258465\n",
      "Epoch: 57, Loss: 0.7267586967080004\n",
      "Epoch: 58, Loss: 0.7260622372333374\n",
      "Epoch: 59, Loss: 0.7246377722584401\n",
      "Epoch: 60, Loss: 0.7282297647282205\n",
      "Epoch: 60, Validation Loss:0.6943149583724638, Accuracy: 0.5026041666666666, accurate: 193, total: 384\n",
      "Validation accuracy did not improve for 9 epochs.\n",
      "Epoch: 61, Loss: 0.7262534819353648\n",
      "Epoch: 62, Loss: 0.723474396125379\n",
      "Epoch: 63, Loss: 0.7307734318698446\n",
      "Epoch: 64, Loss: 0.7252909577693101\n",
      "Epoch: 65, Loss: 0.731671908232211\n",
      "Epoch: 65, Validation Loss:0.6923380972196659, Accuracy: 0.5130208333333334, accurate: 197, total: 384\n",
      "Validation accuracy did not improve for 10 epochs.\n",
      "Epoch: 66, Loss: 0.733764463380255\n",
      "Epoch: 67, Loss: 0.734288665379777\n",
      "Epoch: 68, Loss: 0.7236443808543738\n",
      "Epoch: 69, Loss: 0.7256148323692657\n",
      "Epoch: 70, Loss: 0.7255783491267788\n",
      "Epoch: 70, Validation Loss:0.6956420037895441, Accuracy: 0.5052083333333334, accurate: 194, total: 384\n",
      "Validation accuracy did not improve for 11 epochs.\n",
      "Epoch: 71, Loss: 0.7302863150317636\n",
      "Epoch: 72, Loss: 0.7246031811357372\n",
      "Epoch: 73, Loss: 0.7330997423268855\n",
      "Epoch: 74, Loss: 0.7282670526937755\n",
      "Epoch: 75, Loss: 0.7257522110723786\n",
      "Epoch: 75, Validation Loss:0.6919499537907541, Accuracy: 0.515625, accurate: 198, total: 384\n",
      "Validation accuracy did not improve for 12 epochs.\n",
      "Epoch: 76, Loss: 0.726092608395481\n",
      "Epoch: 77, Loss: 0.737112117793273\n",
      "Epoch: 78, Loss: 0.7389206250060212\n",
      "Epoch: 79, Loss: 0.7323869014572766\n",
      "Epoch: 80, Loss: 0.7281750379889099\n",
      "Epoch: 80, Validation Loss:0.6900229143599669, Accuracy: 0.5390625, accurate: 207, total: 384\n",
      "Epoch: 81, Loss: 0.7249808593618648\n",
      "Epoch: 82, Loss: 0.7290236230560199\n",
      "Epoch: 83, Loss: 0.7286945351709923\n",
      "Epoch: 84, Loss: 0.724518530536443\n",
      "Epoch: 85, Loss: 0.7181894504982564\n",
      "Epoch: 85, Validation Loss:0.6957036873015264, Accuracy: 0.5052083333333334, accurate: 194, total: 384\n",
      "Validation accuracy did not improve for 1 epochs.\n",
      "Epoch: 86, Loss: 0.7241742767876497\n",
      "Epoch: 87, Loss: 0.7283472472594844\n",
      "Epoch: 88, Loss: 0.7242098624076418\n",
      "Epoch: 89, Loss: 0.7283755742461869\n",
      "Epoch: 90, Loss: 0.7235485463213451\n",
      "Epoch: 90, Validation Loss:0.6919738349194328, Accuracy: 0.5208333333333334, accurate: 200, total: 384\n",
      "Validation accuracy did not improve for 2 epochs.\n",
      "Epoch: 91, Loss: 0.726854653313273\n",
      "Epoch: 92, Loss: 0.7317554203241512\n",
      "Epoch: 93, Loss: 0.724111027084291\n",
      "Epoch: 94, Loss: 0.7255223321521448\n",
      "Epoch: 95, Loss: 0.7250799228678699\n",
      "Epoch: 95, Validation Loss:0.6925107954690853, Accuracy: 0.5234375, accurate: 201, total: 384\n",
      "Validation accuracy did not improve for 3 epochs.\n",
      "Epoch: 96, Loss: 0.7240602129087266\n",
      "Epoch: 97, Loss: 0.7250652297710379\n",
      "Epoch: 98, Loss: 0.7348602453712374\n",
      "Epoch: 99, Loss: 0.7364739389848654\n",
      "Epoch: 100, Loss: 0.7211619421529273\n",
      "Epoch: 100, Validation Loss:0.6941256845990816, Accuracy: 0.5130208333333334, accurate: 197, total: 384\n",
      "Validation accuracy did not improve for 4 epochs.\n",
      "Epoch: 101, Loss: 0.7252185327621797\n",
      "Epoch: 102, Loss: 0.7324296105653048\n",
      "Epoch: 103, Loss: 0.7232027465339612\n",
      "Epoch: 104, Loss: 0.7253720582876768\n",
      "Epoch: 105, Loss: 0.7258750575905045\n",
      "Epoch: 105, Validation Loss:0.6881038709543645, Accuracy: 0.5364583333333334, accurate: 206, total: 384\n",
      "Validation accuracy did not improve for 5 epochs.\n",
      "Epoch: 106, Loss: 0.727879613198133\n",
      "Epoch: 107, Loss: 0.7269380033809554\n",
      "Epoch: 108, Loss: 0.7252403945765562\n",
      "Epoch: 109, Loss: 0.7265855500612546\n",
      "Epoch: 110, Loss: 0.7298961865922643\n",
      "Epoch: 110, Validation Loss:0.6935291513800621, Accuracy: 0.5130208333333334, accurate: 197, total: 384\n",
      "Validation accuracy did not improve for 6 epochs.\n",
      "Epoch: 111, Loss: 0.7263629631898193\n",
      "Epoch: 112, Loss: 0.7257005192632614\n",
      "Epoch: 113, Loss: 0.7304039070969103\n",
      "Epoch: 114, Loss: 0.7298889166537534\n",
      "Epoch: 115, Loss: 0.7255599030845419\n",
      "Epoch: 115, Validation Loss:0.6919423087189595, Accuracy: 0.4973958333333333, accurate: 191, total: 384\n",
      "Validation accuracy did not improve for 7 epochs.\n",
      "Epoch: 116, Loss: 0.7304048152916409\n",
      "Epoch: 117, Loss: 0.7280089537509613\n",
      "Epoch: 118, Loss: 0.7275723076977387\n",
      "Epoch: 119, Loss: 0.7356341829764899\n",
      "Epoch: 120, Loss: 0.7293839803120742\n",
      "Epoch: 120, Validation Loss:0.6914638563369712, Accuracy: 0.5052083333333334, accurate: 194, total: 384\n",
      "Validation accuracy did not improve for 8 epochs.\n",
      "Epoch: 121, Loss: 0.7279205711696435\n",
      "Epoch: 122, Loss: 0.7214332673077782\n",
      "Epoch: 123, Loss: 0.730451113363314\n",
      "Epoch: 124, Loss: 0.7216933095758712\n",
      "Epoch: 125, Loss: 0.7271719434540029\n",
      "Epoch: 125, Validation Loss:0.6931236128633221, Accuracy: 0.515625, accurate: 198, total: 384\n",
      "Validation accuracy did not improve for 9 epochs.\n",
      "Epoch: 126, Loss: 0.7278110204885403\n",
      "Epoch: 127, Loss: 0.7304511418117693\n",
      "Epoch: 128, Loss: 0.7248159503840186\n",
      "Epoch: 129, Loss: 0.723332547392972\n",
      "Epoch: 130, Loss: 0.7299475572589371\n",
      "Epoch: 130, Validation Loss:0.6949023393293222, Accuracy: 0.4973958333333333, accurate: 191, total: 384\n",
      "Validation accuracy did not improve for 10 epochs.\n",
      "Epoch: 131, Loss: 0.7235569951072749\n",
      "Epoch: 132, Loss: 0.7250656598178601\n",
      "Epoch: 133, Loss: 0.7270163904565076\n",
      "Epoch: 134, Loss: 0.734039022449266\n",
      "Epoch: 135, Loss: 0.7288569727119196\n",
      "Epoch: 135, Validation Loss:0.693157136750718, Accuracy: 0.5208333333333334, accurate: 200, total: 384\n",
      "Validation accuracy did not improve for 11 epochs.\n",
      "Epoch: 136, Loss: 0.7302768736456832\n",
      "Epoch: 137, Loss: 0.7313645045031552\n",
      "Epoch: 138, Loss: 0.731195666189133\n",
      "Epoch: 139, Loss: 0.7335052542146985\n",
      "Epoch: 140, Loss: 0.7264214245326541\n",
      "Epoch: 140, Validation Loss:0.6921880183120569, Accuracy: 0.5182291666666666, accurate: 199, total: 384\n",
      "Validation accuracy did not improve for 12 epochs.\n",
      "Epoch: 141, Loss: 0.7307251428633377\n",
      "Epoch: 142, Loss: 0.7240044468910329\n",
      "Epoch: 143, Loss: 0.7293781164723138\n",
      "Epoch: 144, Loss: 0.7237910033824543\n",
      "Epoch: 145, Loss: 0.7241699251484264\n",
      "Epoch: 145, Validation Loss:0.6896662471505502, Accuracy: 0.5104166666666666, accurate: 196, total: 384\n",
      "Validation accuracy did not improve for 13 epochs.\n",
      "Epoch: 146, Loss: 0.7183396485030513\n",
      "Epoch: 147, Loss: 0.725525683550923\n",
      "Epoch: 148, Loss: 0.7216404023821708\n",
      "Epoch: 149, Loss: 0.7267246173211822\n",
      "Epoch: 150, Loss: 0.7351367461615829\n",
      "Epoch: 150, Validation Loss:0.6918463742670914, Accuracy: 0.515625, accurate: 198, total: 384\n",
      "Validation accuracy did not improve for 14 epochs.\n",
      "Epoch: 151, Loss: 0.7276257826532755\n",
      "Epoch: 152, Loss: 0.7292000368858377\n",
      "Epoch: 153, Loss: 0.7302794127414624\n",
      "Epoch: 154, Loss: 0.7207379038531885\n",
      "Epoch: 155, Loss: 0.7228299863981428\n",
      "Epoch: 155, Validation Loss:0.694887839568158, Accuracy: 0.4713541666666667, accurate: 181, total: 384\n",
      "Validation accuracy did not improve for 15 epochs.\n",
      "Epoch: 156, Loss: 0.726512135748096\n",
      "Epoch: 157, Loss: 0.7303580672445672\n",
      "Epoch: 158, Loss: 0.7230950123889165\n",
      "Epoch: 159, Loss: 0.7291378077395536\n",
      "Epoch: 160, Loss: 0.7275615365554889\n",
      "Epoch: 160, Validation Loss:0.6895080793959399, Accuracy: 0.515625, accurate: 198, total: 384\n",
      "Validation accuracy did not improve for 16 epochs.\n",
      "Epoch: 161, Loss: 0.7295621291613551\n",
      "Epoch: 162, Loss: 0.7298613420808343\n",
      "Epoch: 163, Loss: 0.7222075227958461\n",
      "Epoch: 164, Loss: 0.7264883581169501\n",
      "Epoch: 165, Loss: 0.7184723785181565\n",
      "Epoch: 165, Validation Loss:0.6926497668027878, Accuracy: 0.5026041666666666, accurate: 193, total: 384\n",
      "Validation accuracy did not improve for 17 epochs.\n",
      "Epoch: 166, Loss: 0.723415475545658\n",
      "Epoch: 167, Loss: 0.7298363147613903\n",
      "Epoch: 168, Loss: 0.7239641446827186\n",
      "Epoch: 169, Loss: 0.7298109078331402\n",
      "Epoch: 170, Loss: 0.7269554186905561\n",
      "Epoch: 170, Validation Loss:0.6974464428300658, Accuracy: 0.5364583333333334, accurate: 206, total: 384\n",
      "Validation accuracy did not improve for 18 epochs.\n",
      "Epoch: 171, Loss: 0.722688725390644\n",
      "Epoch: 172, Loss: 0.7220308538871231\n",
      "Epoch: 173, Loss: 0.7231336188399129\n",
      "Epoch: 174, Loss: 0.7250413407261173\n",
      "Epoch: 175, Loss: 0.7247981940465117\n",
      "Epoch: 175, Validation Loss:0.6920316230195264, Accuracy: 0.5208333333333334, accurate: 200, total: 384\n",
      "Validation accuracy did not improve for 19 epochs.\n",
      "Epoch: 176, Loss: 0.7322097174236896\n",
      "Epoch: 177, Loss: 0.7288790030611886\n",
      "Epoch: 178, Loss: 0.7233986950876122\n",
      "Epoch: 179, Loss: 0.734794272178853\n",
      "Epoch: 180, Loss: 0.7247782340618195\n",
      "Epoch: 180, Validation Loss:0.6949157663621008, Accuracy: 0.515625, accurate: 198, total: 384\n",
      "Validation accuracy did not improve for 20 epochs.\n",
      "Epoch: 181, Loss: 0.7257932716877096\n",
      "Epoch: 182, Loss: 0.7366927284747362\n",
      "Epoch: 183, Loss: 0.7240041131843571\n",
      "Epoch: 184, Loss: 0.7249441308797233\n",
      "Epoch: 185, Loss: 0.7274863353619972\n",
      "Epoch: 185, Validation Loss:0.6915826291466752, Accuracy: 0.5078125, accurate: 195, total: 384\n",
      "Validation accuracy did not improve for 21 epochs.\n",
      "Epoch: 186, Loss: 0.728748994672464\n",
      "Epoch: 187, Loss: 0.7272514946827734\n",
      "Epoch: 188, Loss: 0.7339899263482679\n",
      "Epoch: 189, Loss: 0.726045325346705\n",
      "Epoch: 190, Loss: 0.7274062173717\n",
      "Epoch: 190, Validation Loss:0.6912668938748538, Accuracy: 0.5208333333333334, accurate: 200, total: 384\n",
      "Validation accuracy did not improve for 22 epochs.\n",
      "Epoch: 191, Loss: 0.7225768433331892\n",
      "Epoch: 192, Loss: 0.734758231219732\n",
      "Epoch: 193, Loss: 0.7293351154636454\n",
      "Epoch: 194, Loss: 0.728460992384633\n",
      "Epoch: 195, Loss: 0.7333002582157927\n",
      "Epoch: 195, Validation Loss:0.6864454920093218, Accuracy: 0.5234375, accurate: 201, total: 384\n",
      "Validation accuracy did not improve for 23 epochs.\n",
      "Epoch: 196, Loss: 0.7213712099215223\n",
      "Epoch: 197, Loss: 0.7254460540393161\n",
      "Epoch: 198, Loss: 0.7345696224451617\n",
      "Epoch: 199, Loss: 0.7347395104804525\n",
      "Epoch: 200, Loss: 0.7253511331251098\n",
      "Epoch: 200, Validation Loss:0.6936467834748328, Accuracy: 0.5026041666666666, accurate: 193, total: 384\n",
      "Validation accuracy did not improve for 24 epochs.\n",
      "Epoch: 201, Loss: 0.7249367294177689\n",
      "Epoch: 202, Loss: 0.7284979781618824\n",
      "Epoch: 203, Loss: 0.7312350881510172\n",
      "Epoch: 204, Loss: 0.7361328567516197\n",
      "Epoch: 205, Loss: 0.7283104285597801\n",
      "Epoch: 205, Validation Loss:0.694005947560072, Accuracy: 0.5078125, accurate: 195, total: 384\n",
      "Validation accuracy did not improve for 25 epochs.\n",
      "Epoch: 206, Loss: 0.7307272791448567\n",
      "Epoch: 207, Loss: 0.7280690199695528\n",
      "Epoch: 208, Loss: 0.7236870313753132\n",
      "Epoch: 209, Loss: 0.7247661357676541\n",
      "Epoch: 210, Loss: 0.7222760197258106\n",
      "Epoch: 210, Validation Loss:0.6883373294646541, Accuracy: 0.5390625, accurate: 207, total: 384\n",
      "Validation accuracy did not improve for 26 epochs.\n",
      "Epoch: 211, Loss: 0.7271878608433461\n",
      "Epoch: 212, Loss: 0.722953172598931\n",
      "Epoch: 213, Loss: 0.7267314742181312\n",
      "Epoch: 214, Loss: 0.7309447275819602\n",
      "Epoch: 215, Loss: 0.7306971056559296\n",
      "Epoch: 215, Validation Loss:0.6912139553266267, Accuracy: 0.5104166666666666, accurate: 196, total: 384\n",
      "Validation accuracy did not improve for 27 epochs.\n",
      "Epoch: 216, Loss: 0.7173048153657604\n",
      "Epoch: 217, Loss: 0.7241661494632287\n",
      "Epoch: 218, Loss: 0.7259665917674148\n",
      "Epoch: 219, Loss: 0.731611466569895\n",
      "Epoch: 220, Loss: 0.7273447519616673\n",
      "Epoch: 220, Validation Loss:0.6941205950764319, Accuracy: 0.4791666666666667, accurate: 184, total: 384\n",
      "Validation accuracy did not improve for 28 epochs.\n",
      "Epoch: 221, Loss: 0.7205443730129412\n",
      "Epoch: 222, Loss: 0.7310572044440994\n",
      "Epoch: 223, Loss: 0.7206105907896051\n",
      "Epoch: 224, Loss: 0.7222343282601623\n",
      "Epoch: 225, Loss: 0.7222770692918588\n",
      "Epoch: 225, Validation Loss:0.6904293890111148, Accuracy: 0.53125, accurate: 204, total: 384\n",
      "Validation accuracy did not improve for 29 epochs.\n",
      "Epoch: 226, Loss: 0.7246197871691374\n",
      "Epoch: 227, Loss: 0.7239626204870917\n",
      "Epoch: 228, Loss: 0.7240586249864902\n",
      "Epoch: 229, Loss: 0.7283787238066671\n",
      "Epoch: 230, Loss: 0.7340218905980388\n",
      "Epoch: 230, Validation Loss:0.6927735482652982, Accuracy: 0.5234375, accurate: 201, total: 384\n",
      "Validation accuracy did not improve for 30 epochs.\n",
      "Epoch: 231, Loss: 0.7231190218503967\n",
      "Epoch: 232, Loss: 0.7253495234288965\n",
      "Epoch: 233, Loss: 0.7285466752601443\n",
      "Epoch: 234, Loss: 0.7210477615741117\n",
      "Epoch: 235, Loss: 0.7233068682425828\n",
      "Epoch: 235, Validation Loss:0.6947027742862701, Accuracy: 0.5104166666666666, accurate: 196, total: 384\n",
      "Validation accuracy did not improve for 31 epochs.\n",
      "Epoch: 236, Loss: 0.7247814771512316\n",
      "Epoch: 237, Loss: 0.7244889920370446\n",
      "Epoch: 238, Loss: 0.7328404395892802\n",
      "Epoch: 239, Loss: 0.7261153386767816\n",
      "Epoch: 240, Loss: 0.7261500030117868\n",
      "Epoch: 240, Validation Loss:0.6921862550079823, Accuracy: 0.5260416666666666, accurate: 202, total: 384\n",
      "Validation accuracy did not improve for 32 epochs.\n",
      "Epoch: 241, Loss: 0.7279329956250472\n",
      "Epoch: 242, Loss: 0.7323977859449331\n",
      "Epoch: 243, Loss: 0.7265128635939349\n",
      "Epoch: 244, Loss: 0.721211043117499\n",
      "Epoch: 245, Loss: 0.7246280453185102\n",
      "Epoch: 245, Validation Loss:0.6884733422969779, Accuracy: 0.5234375, accurate: 201, total: 384\n",
      "Validation accuracy did not improve for 33 epochs.\n",
      "Epoch: 246, Loss: 0.7285213510298895\n",
      "Epoch: 247, Loss: 0.7301570374208192\n",
      "Epoch: 248, Loss: 0.7266167838264395\n",
      "Epoch: 249, Loss: 0.7285283068367453\n",
      "Epoch: 250, Loss: 0.7298985601540793\n",
      "Epoch: 250, Validation Loss:0.6930823686222235, Accuracy: 0.5078125, accurate: 195, total: 384\n",
      "Validation accuracy did not improve for 34 epochs.\n",
      "Epoch: 251, Loss: 0.7276549201838121\n",
      "Epoch: 252, Loss: 0.7216710018304487\n",
      "Epoch: 253, Loss: 0.7253844934791602\n",
      "Epoch: 254, Loss: 0.7261906332181146\n",
      "Epoch: 255, Loss: 0.73015985439566\n",
      "Epoch: 255, Validation Loss:0.692917633180817, Accuracy: 0.5260416666666666, accurate: 202, total: 384\n",
      "Validation accuracy did not improve for 35 epochs.\n",
      "Epoch: 256, Loss: 0.7247183982938252\n",
      "Epoch: 257, Loss: 0.722672540522008\n",
      "Epoch: 258, Loss: 0.72772865143984\n",
      "Epoch: 259, Loss: 0.7315562331393637\n",
      "Epoch: 260, Loss: 0.7230210472588185\n",
      "Epoch: 260, Validation Loss:0.6913285832852125, Accuracy: 0.515625, accurate: 198, total: 384\n",
      "Validation accuracy did not improve for 36 epochs.\n",
      "Epoch: 261, Loss: 0.7251841655104525\n",
      "Epoch: 262, Loss: 0.7273256905652858\n",
      "Epoch: 263, Loss: 0.7259257243670247\n",
      "Epoch: 264, Loss: 0.7261415402991352\n",
      "Epoch: 265, Loss: 0.7247251014190691\n",
      "Epoch: 265, Validation Loss:0.6941280777876576, Accuracy: 0.4947916666666667, accurate: 190, total: 384\n",
      "Validation accuracy did not improve for 37 epochs.\n",
      "Epoch: 266, Loss: 0.729162491409591\n",
      "Epoch: 267, Loss: 0.7244752195180842\n",
      "Epoch: 268, Loss: 0.7191749444162404\n",
      "Epoch: 269, Loss: 0.7210078542465689\n",
      "Epoch: 270, Loss: 0.7230480023383818\n",
      "Epoch: 270, Validation Loss:0.6914266829068462, Accuracy: 0.5208333333333334, accurate: 200, total: 384\n",
      "Validation accuracy did not improve for 38 epochs.\n",
      "Epoch: 271, Loss: 0.7265486335123165\n",
      "Epoch: 272, Loss: 0.7261770031483913\n",
      "Epoch: 273, Loss: 0.734666621312499\n",
      "Epoch: 274, Loss: 0.7241088273521099\n",
      "Epoch: 275, Loss: 0.723746539335008\n",
      "Epoch: 275, Validation Loss:0.6919093628724416, Accuracy: 0.5052083333333334, accurate: 194, total: 384\n",
      "Validation accuracy did not improve for 39 epochs.\n",
      "Epoch: 276, Loss: 0.723075099306664\n",
      "Epoch: 277, Loss: 0.7302021383204393\n",
      "Epoch: 278, Loss: 0.7269276016633268\n",
      "Epoch: 279, Loss: 0.7321923913399654\n",
      "Epoch: 280, Loss: 0.7342036765896611\n",
      "Epoch: 280, Validation Loss:0.6957931984215975, Accuracy: 0.515625, accurate: 198, total: 384\n",
      "Validation accuracy did not improve for 40 epochs.\n",
      "Epoch: 281, Loss: 0.7246950424828187\n",
      "Epoch: 282, Loss: 0.7276434952186214\n",
      "Epoch: 283, Loss: 0.7228616896361388\n",
      "Epoch: 284, Loss: 0.7229900692796541\n",
      "Epoch: 285, Loss: 0.7249228459334484\n",
      "Epoch: 285, Validation Loss:0.6919014042553803, Accuracy: 0.5130208333333334, accurate: 197, total: 384\n",
      "Validation accuracy did not improve for 41 epochs.\n",
      "Epoch: 286, Loss: 0.7238417681892989\n",
      "Epoch: 287, Loss: 0.7283549414757915\n",
      "Epoch: 288, Loss: 0.7280863143710626\n",
      "Epoch: 289, Loss: 0.7249738291526834\n",
      "Epoch: 290, Loss: 0.7191100224369654\n",
      "Epoch: 290, Validation Loss:0.6912043749665221, Accuracy: 0.5078125, accurate: 195, total: 384\n",
      "Validation accuracy did not improve for 42 epochs.\n",
      "Epoch: 291, Loss: 0.7288004568588264\n",
      "Epoch: 292, Loss: 0.7240139225894516\n",
      "Epoch: 293, Loss: 0.7248289501466961\n",
      "Epoch: 294, Loss: 0.7307939697401943\n",
      "Epoch: 295, Loss: 0.7258460587063046\n",
      "Epoch: 295, Validation Loss:0.6912204953841865, Accuracy: 0.5078125, accurate: 195, total: 384\n",
      "Validation accuracy did not improve for 43 epochs.\n",
      "Epoch: 296, Loss: 0.7330413690285275\n",
      "Epoch: 297, Loss: 0.7271210457439776\n",
      "Epoch: 298, Loss: 0.725139943037734\n",
      "Epoch: 299, Loss: 0.7287925348824097\n",
      "Epoch: 300, Loss: 0.7298830917312039\n",
      "Epoch: 300, Validation Loss:0.695047238531212, Accuracy: 0.515625, accurate: 198, total: 384\n",
      "Validation accuracy did not improve for 44 epochs.\n",
      "Epoch: 301, Loss: 0.7292162288256265\n",
      "Epoch: 302, Loss: 0.7327747001243686\n",
      "Epoch: 303, Loss: 0.7314522771716669\n",
      "Epoch: 304, Loss: 0.7327939056687884\n",
      "Epoch: 305, Loss: 0.7293952647769065\n",
      "Epoch: 305, Validation Loss:0.6918497638156017, Accuracy: 0.5390625, accurate: 207, total: 384\n",
      "Validation accuracy did not improve for 45 epochs.\n",
      "Epoch: 306, Loss: 0.7241544098576048\n",
      "Epoch: 307, Loss: 0.7247241906028379\n",
      "Epoch: 308, Loss: 0.7331676141669353\n",
      "Epoch: 309, Loss: 0.725884358778044\n",
      "Epoch: 310, Loss: 0.7318865306916889\n",
      "Epoch: 310, Validation Loss:0.6912149363197386, Accuracy: 0.5104166666666666, accurate: 196, total: 384\n",
      "Validation accuracy did not improve for 46 epochs.\n",
      "Epoch: 311, Loss: 0.7231851610424066\n",
      "Epoch: 312, Loss: 0.7294642284891948\n",
      "Epoch: 313, Loss: 0.7215940538207414\n",
      "Epoch: 314, Loss: 0.7292978223240761\n",
      "Epoch: 315, Loss: 0.7313051126310947\n",
      "Epoch: 315, Validation Loss:0.6897198415050904, Accuracy: 0.5338541666666666, accurate: 205, total: 384\n",
      "Validation accuracy did not improve for 47 epochs.\n",
      "Epoch: 316, Loss: 0.7334769170351878\n",
      "Epoch: 317, Loss: 0.7283430996661385\n",
      "Epoch: 318, Loss: 0.728066231486284\n",
      "Epoch: 319, Loss: 0.7262686310956875\n",
      "Epoch: 320, Loss: 0.7277236146403959\n",
      "Epoch: 320, Validation Loss:0.6907828304295739, Accuracy: 0.5130208333333334, accurate: 197, total: 384\n",
      "Validation accuracy did not improve for 48 epochs.\n",
      "Epoch: 321, Loss: 0.7265250433615789\n",
      "Epoch: 322, Loss: 0.7224178770039644\n",
      "Epoch: 323, Loss: 0.7262966866018595\n",
      "Epoch: 324, Loss: 0.7230396806607368\n",
      "Epoch: 325, Loss: 0.7314278733985567\n",
      "Epoch: 325, Validation Loss:0.6941894103462497, Accuracy: 0.5078125, accurate: 195, total: 384\n",
      "Validation accuracy did not improve for 49 epochs.\n",
      "Epoch: 326, Loss: 0.7253171045816055\n",
      "Epoch: 327, Loss: 0.7186085076278282\n",
      "Epoch: 328, Loss: 0.7254679543828523\n",
      "Epoch: 329, Loss: 0.727938073799359\n",
      "Epoch: 330, Loss: 0.73053762872048\n",
      "Epoch: 330, Validation Loss:0.690784640610218, Accuracy: 0.5234375, accurate: 201, total: 384\n",
      "Validation accuracy did not improve for 50 epochs.\n",
      "Early stopping triggered after 50 epochs with no improvement. Best accuracy: 0.5390625 at epoch 80\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "best_val_accuracy = 0\n",
    "best_accu_epoch = 0\n",
    "print(model.__class__.__name__)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    print(f'Epoch: {epoch}, Loss: {avg_train_loss}')\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        accurate = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader, 0):\n",
    "                inputs, labels = data\n",
    "                outputs = model(inputs)\n",
    "                # print(outputs)\n",
    "                # output = torch.argmax(outputs, 1)\n",
    "                output = torch.round(outputs)\n",
    "                accurate += torch.sum(output == labels).item()\n",
    "                \n",
    "                val_loss = criterion(outputs, labels)\n",
    "                total_loss += val_loss.item()\n",
    "        \n",
    "        avg_val_loss = total_loss / len(val_loader)\n",
    "        accuracy = accurate / len(y_val)\n",
    "        \n",
    "        # scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch: {epoch}, Validation Loss:{avg_val_loss}, Accuracy: {accuracy}, accurate: {accurate}, total: {len(y_val)}')\n",
    "        \n",
    "        # Check if validation loss improved\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            patience_counter = 0  # Reset counter if validation loss improved\n",
    "            best_accu_epoch = epoch\n",
    "            torch.save(model.state_dict(), f'model/best_model_epoch{epoch}.pth')  # Save the best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation accuracy did not improve for {patience_counter} epochs.\")\n",
    "        \n",
    "        #  # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs with no improvement. Best accuracy: {best_val_accuracy} at epoch {best_accu_epoch}\")\n",
    "            break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNClassifier500\n",
      "(960, 500) (960, 1)\n",
      "Test Loss: 0.7012461584061385, Accuracy: 0.503125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/1546446104.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "#### BEST EPOCH 125 for 500 bits input\n",
    "# Calculate the accuracy of the model\n",
    "best_epoch = 125\n",
    "model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n",
    "model.eval()\n",
    "test_data = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())\n",
    "test_loader = DataLoader(test_data, batch_size=bactch_size, shuffle=True)\n",
    "print(model.__class__.__name__)\n",
    "print(X_test.shape, y_test.shape)\n",
    "total_loss = 0.0\n",
    "accurate = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        output = torch.round(outputs)\n",
    "        accurate += torch.sum(output == labels).item()\n",
    "        \n",
    "        test_loss = criterion(outputs, labels)\n",
    "        total_loss += test_loss.item()\n",
    "\n",
    "avg_test_loss = total_loss / len(test_loader)\n",
    "accuracy = accurate / len(y_test)\n",
    "print(f'Test Loss: {avg_test_loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNClassifier800\n",
      "(600, 800) (600, 1)\n",
      "Test Loss: 0.6935586561759313, Accuracy: 0.5116666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/3978821641.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "#### BEST EPOCH 105 for 800 bits input\n",
    "# Calculate the accuracy of the model\n",
    "best_epoch = 105\n",
    "model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n",
    "model.eval()\n",
    "test_data = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())\n",
    "test_loader = DataLoader(test_data, batch_size=bactch_size, shuffle=True)\n",
    "print(model.__class__.__name__)\n",
    "print(X_test.shape, y_test.shape)\n",
    "total_loss = 0.0\n",
    "accurate = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        output = torch.round(outputs)\n",
    "        accurate += torch.sum(output == labels).item()\n",
    "        \n",
    "        test_loss = criterion(outputs, labels)\n",
    "        total_loss += test_loss.item()\n",
    "\n",
    "avg_test_loss = total_loss / len(test_loader)\n",
    "accuracy = accurate / len(y_test)\n",
    "print(f'Test Loss: {avg_test_loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNClassifier400\n",
      "(1200, 400) (1200, 1)\n",
      "Test Loss: 0.691421952744325, Accuracy: 0.5316666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/2366530904.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "#### BEST EPOCH 145 for 400 bits input\n",
    "# Calculate the accuracy of the model\n",
    "best_epoch = 145\n",
    "model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n",
    "model.eval()\n",
    "test_data = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())\n",
    "test_loader = DataLoader(test_data, batch_size=bactch_size, shuffle=True)\n",
    "print(model.__class__.__name__)\n",
    "print(X_test.shape, y_test.shape)\n",
    "total_loss = 0.0\n",
    "accurate = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        output = torch.round(outputs)\n",
    "        accurate += torch.sum(output == labels).item()\n",
    "        \n",
    "        test_loss = criterion(outputs, labels)\n",
    "        total_loss += test_loss.item()\n",
    "\n",
    "avg_test_loss = total_loss / len(test_loader)\n",
    "accuracy = accurate / len(y_test)\n",
    "print(f'Test Loss: {avg_test_loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNClassifier200\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8x200 and 100x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[197], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[195], line 53\u001b[0m, in \u001b[0;36mRNClassifier200.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 53\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x200 and 100x64)"
     ]
    }
   ],
   "source": [
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "best_val_accuracy = 0\n",
    "best_accu_epoch = 0\n",
    "print(model.__class__.__name__)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    print(f'Epoch: {epoch}, Loss: {avg_train_loss}')\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        accurate = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader, 0):\n",
    "                inputs, labels = data\n",
    "                outputs = model(inputs)\n",
    "                # print(outputs)\n",
    "                # output = torch.argmax(outputs, 1)\n",
    "                output = torch.round(outputs)\n",
    "                accurate += torch.sum(output == labels).item()\n",
    "                \n",
    "                val_loss = criterion(outputs, labels)\n",
    "                total_loss += val_loss.item()\n",
    "        \n",
    "        avg_val_loss = total_loss / len(val_loader)\n",
    "        accuracy = accurate / len(y_val)\n",
    "        \n",
    "        # scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch: {epoch}, Validation Loss:{avg_val_loss}, Accuracy: {accuracy}, accurate: {accurate}, total: {len(y_val)}')\n",
    "        \n",
    "        # Check if validation loss improved\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            patience_counter = 0  # Reset counter if validation loss improved\n",
    "            best_accu_epoch = epoch\n",
    "            torch.save(model.state_dict(), f'model/best_model_epoch{epoch}.pth')  # Save the best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation accuracy did not improve for {patience_counter} epochs.\")\n",
    "        \n",
    "        #  # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs with no improvement. Best accuracy: {best_val_accuracy} at epoch {best_accu_epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6979442413647969, Accuracy: 0.5225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/719008882.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### BEST EPOCH 60 for 100 bits input\n",
    "# Calculate the accuracy of the model\n",
    "best_epoch = 60\n",
    "model.load_state_dict(torch.load(f'model/best_model_epoch{best_epoch}.pth'))\n",
    "model.eval()\n",
    "test_data = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())\n",
    "test_loader = DataLoader(test_data, batch_size=bactch_size, shuffle=True)\n",
    "\n",
    "total_loss = 0.0\n",
    "accurate = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        output = torch.round(outputs)\n",
    "        accurate += torch.sum(output == labels).item()\n",
    "        \n",
    "        test_loss = criterion(outputs, labels)\n",
    "        total_loss += test_loss.item()\n",
    "\n",
    "avg_test_loss = total_loss / len(test_loader)\n",
    "accuracy = accurate / len(y_test)\n",
    "print(f'Test Loss: {avg_test_loss}, Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wigglerng-jNs1t9jR-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
