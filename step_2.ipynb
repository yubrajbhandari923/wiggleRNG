{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries (install in your environment first)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log2, sqrt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000101000100111111110011011110111101101010111...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0100101111010000110010000101001110101001001010...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000101010100100011100101111011111001110011101...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0111101100010110010000011111111001110001100110...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1111100000011110111111111111101001100100011010...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  label\n",
       "0  0000101000100111111110011011110111101101010111...      1\n",
       "1  0100101111010000110010000101001110101001001010...      1\n",
       "2  1000101010100100011100101111011111001110011101...      1\n",
       "3  0111101100010110010000011111111001110001100110...      1\n",
       "4  1111100000011110111111111111101001100100011010...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filePath = 'QRNGvsPRNG_TrainingData.txt'\n",
    "df = pd.read_csv(data_filePath, sep=' ',header=None, dtype={\"data\": str, \"label\": np.int64})\n",
    "df.columns = [\"data\", \"label\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Labels and Train_Test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    12000\n",
       "2    12000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].apply(lambda x: x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df['data'].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Split the each string of X into a list of characters, and convert them to integers\n",
    "X = np.array([list(map(int, list(x))) for x in X])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, shuffle=True, random_state=42)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((17280, 100), (17280, 1)),\n",
       " ((1920, 100), (1920, 1)),\n",
       " ((4800, 100), (4800, 1)))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]),\n",
       " array([1]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[10], y_train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(100, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 64)   \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "    \n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = F.relu(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        # return x\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "model = RNClassifier()\n",
    "# Split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/2757594806.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train).float()\n",
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/2757594806.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train).float()\n",
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/2757594806.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val).float()\n",
      "/var/folders/qv/f4vv6vrd3qn96rx6p0mvm4dc0000gn/T/ipykernel_51659/2757594806.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val = torch.tensor(y_val).float()\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=100, verbose=True, factor=0.5)\n",
    "\n",
    "epochs = 10000\n",
    "bactch_size = 4\n",
    "patience =100\n",
    "\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_val = torch.tensor(X_val).float()\n",
    "y_val = torch.tensor(y_val).float()\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=bactch_size, shuffle=True)\n",
    "\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_data, batch_size=bactch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6836093868922304\n",
      "Epoch: 0, Validation Loss:0.6947738741834958, Accuracy: 0.5145833333333333, accurate: 988, total: 1920\n",
      "Epoch: 1, Loss: 0.6845677883812675\n",
      "Epoch: 2, Loss: 0.6825451243530821\n",
      "Epoch: 3, Loss: 0.6818700810480449\n",
      "Epoch: 4, Loss: 0.6831432122185275\n",
      "Epoch: 5, Loss: 0.6841333798926186\n",
      "Epoch: 5, Validation Loss:0.6943737986187141, Accuracy: 0.5166666666666667, accurate: 992, total: 1920\n",
      "Epoch: 6, Loss: 0.6831708277541179\n",
      "Epoch: 7, Loss: 0.6813423861025109\n",
      "Epoch: 8, Loss: 0.6834004921493707\n",
      "Epoch: 9, Loss: 0.6839415942767152\n",
      "Epoch: 10, Loss: 0.685672658123076\n",
      "Epoch: 10, Validation Loss:0.6927746662249168, Accuracy: 0.515625, accurate: 990, total: 1920\n",
      "Validation accuracy did not improve for 1 epochs.\n",
      "Epoch: 11, Loss: 0.6843576080821179\n",
      "Epoch: 12, Loss: 0.6829402519734922\n",
      "Epoch: 13, Loss: 0.6846776731726196\n",
      "Epoch: 14, Loss: 0.6825897476325432\n",
      "Epoch: 15, Loss: 0.6837543761426652\n",
      "Epoch: 15, Validation Loss:0.6938212599605322, Accuracy: 0.5151041666666667, accurate: 989, total: 1920\n",
      "Validation accuracy did not improve for 2 epochs.\n",
      "Epoch: 16, Loss: 0.6833207090833673\n",
      "Epoch: 17, Loss: 0.6841218215861806\n",
      "Epoch: 18, Loss: 0.68294507285787\n",
      "Epoch: 19, Loss: 0.6830180761024908\n",
      "Epoch: 20, Loss: 0.6829022853600758\n",
      "Epoch: 20, Validation Loss:0.6949201154212157, Accuracy: 0.5098958333333333, accurate: 979, total: 1920\n",
      "Validation accuracy did not improve for 3 epochs.\n",
      "Epoch: 21, Loss: 0.6857532198437386\n",
      "Epoch: 22, Loss: 0.6822997928907474\n",
      "Epoch: 23, Loss: 0.6832197323096572\n",
      "Epoch: 24, Loss: 0.6833642013233017\n",
      "Epoch: 25, Loss: 0.684012194060617\n",
      "Epoch: 25, Validation Loss:0.6935721014936765, Accuracy: 0.5145833333333333, accurate: 988, total: 1920\n",
      "Validation accuracy did not improve for 4 epochs.\n",
      "Epoch: 26, Loss: 0.6822428081805507\n",
      "Epoch: 27, Loss: 0.6837266671436805\n",
      "Epoch: 28, Loss: 0.684051294197087\n",
      "Epoch: 29, Loss: 0.685385147696016\n",
      "Epoch: 30, Loss: 0.6832476243790653\n",
      "Epoch: 30, Validation Loss:0.6937921158969402, Accuracy: 0.5171875, accurate: 993, total: 1920\n",
      "Epoch: 31, Loss: 0.6824081795083152\n",
      "Epoch: 32, Loss: 0.6841471821483639\n",
      "Epoch: 33, Loss: 0.6835265642101014\n",
      "Epoch: 34, Loss: 0.6831266206171778\n",
      "Epoch: 35, Loss: 0.6828388386371511\n",
      "Epoch: 35, Validation Loss:0.6942420094584425, Accuracy: 0.5109375, accurate: 981, total: 1920\n",
      "Validation accuracy did not improve for 1 epochs.\n",
      "Epoch: 36, Loss: 0.682356435150184\n",
      "Epoch: 37, Loss: 0.6827876560183035\n",
      "Epoch: 38, Loss: 0.6830892873720991\n",
      "Epoch: 39, Loss: 0.682544707989803\n",
      "Epoch: 40, Loss: 0.6821662311200742\n",
      "Epoch: 40, Validation Loss:0.695058448985219, Accuracy: 0.5161458333333333, accurate: 991, total: 1920\n",
      "Validation accuracy did not improve for 2 epochs.\n",
      "Epoch: 41, Loss: 0.6823994320161917\n",
      "Epoch: 42, Loss: 0.6855609944711129\n",
      "Epoch: 43, Loss: 0.6836261936221961\n",
      "Epoch: 44, Loss: 0.6822283892650847\n",
      "Epoch: 45, Loss: 0.6835061498400238\n",
      "Epoch: 45, Validation Loss:0.6947979390621185, Accuracy: 0.5161458333333333, accurate: 991, total: 1920\n",
      "Validation accuracy did not improve for 3 epochs.\n",
      "Epoch: 46, Loss: 0.682199530068923\n",
      "Epoch: 47, Loss: 0.6827369909112652\n",
      "Epoch: 48, Loss: 0.6847449873608572\n",
      "Epoch: 49, Loss: 0.6848761104185273\n",
      "Epoch: 50, Loss: 0.6821403117643462\n",
      "Epoch: 50, Validation Loss:0.6948111576338609, Accuracy: 0.5161458333333333, accurate: 991, total: 1920\n",
      "Validation accuracy did not improve for 4 epochs.\n",
      "Epoch: 51, Loss: 0.6824136869843911\n",
      "Epoch: 52, Loss: 0.6818456648262563\n",
      "Epoch: 53, Loss: 0.6836912162394987\n",
      "Epoch: 54, Loss: 0.6837405574473517\n",
      "Epoch: 55, Loss: 0.6836335515189502\n",
      "Epoch: 55, Validation Loss:0.6942942970742781, Accuracy: 0.5161458333333333, accurate: 991, total: 1920\n",
      "Validation accuracy did not improve for 5 epochs.\n",
      "Epoch: 56, Loss: 0.6843572971859464\n",
      "Epoch: 57, Loss: 0.6844846780515379\n",
      "Epoch: 58, Loss: 0.6827571034017537\n",
      "Epoch: 59, Loss: 0.6827413801931672\n",
      "Epoch: 60, Loss: 0.6839822503013744\n",
      "Epoch: 60, Validation Loss:0.6948765027026336, Accuracy: 0.5135416666666667, accurate: 986, total: 1920\n",
      "Validation accuracy did not improve for 6 epochs.\n",
      "Epoch: 61, Loss: 0.6822662443681448\n",
      "Epoch: 62, Loss: 0.6818532240266602\n",
      "Epoch: 63, Loss: 0.6826088330880912\n",
      "Epoch: 64, Loss: 0.6833298941384311\n",
      "Epoch: 65, Loss: 0.6833423239696357\n",
      "Epoch: 65, Validation Loss:0.6936671047161023, Accuracy: 0.5125, accurate: 984, total: 1920\n",
      "Validation accuracy did not improve for 7 epochs.\n",
      "Epoch: 66, Loss: 0.6814229248850434\n",
      "Epoch: 67, Loss: 0.6833869771548995\n",
      "Epoch: 68, Loss: 0.6833928957926454\n",
      "Epoch: 69, Loss: 0.6828000259758146\n",
      "Epoch: 70, Loss: 0.6824726577158328\n",
      "Epoch: 70, Validation Loss:0.6942960392683745, Accuracy: 0.521875, accurate: 1002, total: 1920\n",
      "Epoch: 71, Loss: 0.6827120878906162\n",
      "Epoch: 72, Loss: 0.6832695982936355\n",
      "Epoch: 73, Loss: 0.6812220279817228\n",
      "Epoch: 74, Loss: 0.6816311226812778\n",
      "Epoch: 75, Loss: 0.6837748335950352\n",
      "Epoch: 75, Validation Loss:0.6947249325613181, Accuracy: 0.5177083333333333, accurate: 994, total: 1920\n",
      "Validation accuracy did not improve for 1 epochs.\n",
      "Epoch: 76, Loss: 0.68242574316208\n",
      "Epoch: 77, Loss: 0.6822721303268163\n",
      "Epoch: 78, Loss: 0.6829219514986983\n",
      "Epoch: 79, Loss: 0.6821268641769335\n",
      "Epoch: 80, Loss: 0.6813659923358096\n",
      "Epoch: 80, Validation Loss:0.6948221176862717, Accuracy: 0.5166666666666667, accurate: 992, total: 1920\n",
      "Validation accuracy did not improve for 2 epochs.\n",
      "Epoch: 81, Loss: 0.6827425061276665\n",
      "Epoch: 82, Loss: 0.682151408293457\n",
      "Epoch: 83, Loss: 0.6832324196894963\n",
      "Epoch: 84, Loss: 0.6825020076017136\n",
      "Epoch: 85, Loss: 0.6833517880627402\n",
      "Epoch: 85, Validation Loss:0.6941987055043379, Accuracy: 0.515625, accurate: 990, total: 1920\n",
      "Validation accuracy did not improve for 3 epochs.\n",
      "Epoch: 86, Loss: 0.6835431510099659\n",
      "Epoch: 87, Loss: 0.6841345659353667\n",
      "Epoch: 88, Loss: 0.6844186151883116\n",
      "Epoch: 89, Loss: 0.6826064415551998\n",
      "Epoch: 90, Loss: 0.6820522761593263\n",
      "Epoch: 90, Validation Loss:0.6945441863189141, Accuracy: 0.5208333333333334, accurate: 1000, total: 1920\n",
      "Validation accuracy did not improve for 4 epochs.\n",
      "Epoch: 91, Loss: 0.6844175512660985\n",
      "Epoch: 92, Loss: 0.6827628089666918\n",
      "Epoch: 93, Loss: 0.6830311929669093\n",
      "Epoch: 94, Loss: 0.6815885619639799\n",
      "Epoch: 95, Loss: 0.6833869716911404\n",
      "Epoch: 95, Validation Loss:0.6940380894889434, Accuracy: 0.5255208333333333, accurate: 1009, total: 1920\n",
      "Epoch: 96, Loss: 0.6852986354449833\n",
      "Epoch: 97, Loss: 0.6824279436910594\n",
      "Epoch: 98, Loss: 0.6815627862526863\n",
      "Epoch: 99, Loss: 0.6826744544975184\n",
      "Epoch: 100, Loss: 0.6822677400377062\n",
      "Epoch: 100, Validation Loss:0.6936154752969742, Accuracy: 0.5171875, accurate: 993, total: 1920\n",
      "Validation accuracy did not improve for 1 epochs.\n",
      "Epoch: 101, Loss: 0.6808799156367227\n",
      "Epoch: 102, Loss: 0.6832440657472169\n",
      "Epoch: 103, Loss: 0.6829738248277594\n",
      "Epoch: 104, Loss: 0.6827715061191055\n",
      "Epoch: 105, Loss: 0.681694655686065\n",
      "Epoch: 105, Validation Loss:0.6936894904822111, Accuracy: 0.5114583333333333, accurate: 982, total: 1920\n",
      "Validation accuracy did not improve for 2 epochs.\n",
      "Epoch: 106, Loss: 0.6831180288628848\n",
      "Epoch: 107, Loss: 0.6830266283204158\n",
      "Epoch: 108, Loss: 0.6833220830808083\n",
      "Epoch: 109, Loss: 0.6838204688082139\n",
      "Epoch: 110, Loss: 0.6837001482369723\n",
      "Epoch: 110, Validation Loss:0.6943622808903456, Accuracy: 0.5229166666666667, accurate: 1004, total: 1920\n",
      "Validation accuracy did not improve for 3 epochs.\n",
      "Epoch: 111, Loss: 0.682481152260745\n",
      "Epoch: 112, Loss: 0.6818396304768545\n",
      "Epoch: 113, Loss: 0.6824973237183359\n",
      "Epoch: 114, Loss: 0.6822825143182719\n",
      "Epoch: 115, Loss: 0.6811683339377245\n",
      "Epoch: 115, Validation Loss:0.6940289596716563, Accuracy: 0.50625, accurate: 972, total: 1920\n",
      "Validation accuracy did not improve for 4 epochs.\n",
      "Epoch: 116, Loss: 0.6810365169067626\n",
      "Epoch: 117, Loss: 0.6828972149433361\n",
      "Epoch: 118, Loss: 0.6828813819283689\n",
      "Epoch: 119, Loss: 0.6821703506426678\n",
      "Epoch: 120, Loss: 0.6833044292681195\n",
      "Epoch: 120, Validation Loss:0.6939982357124488, Accuracy: 0.5182291666666666, accurate: 995, total: 1920\n",
      "Validation accuracy did not improve for 5 epochs.\n",
      "Epoch: 121, Loss: 0.6807640592809077\n",
      "Epoch: 122, Loss: 0.6849461103892989\n",
      "Epoch: 123, Loss: 0.680510255811667\n",
      "Epoch: 124, Loss: 0.6812703828637798\n",
      "Epoch: 125, Loss: 0.6825112918184864\n",
      "Epoch: 125, Validation Loss:0.6938041379675269, Accuracy: 0.51875, accurate: 996, total: 1920\n",
      "Validation accuracy did not improve for 6 epochs.\n",
      "Epoch: 126, Loss: 0.6832861933512269\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[105], line 19\u001b[0m, in \u001b[0;36mRNClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:176\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/wigglerng-jNs1t9jR-py3.11/lib/python3.11/site-packages/torch/nn/functional.py:2512\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2510\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2513\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2514\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "best_val_accuracy = 0\n",
    "best_accu_epoch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    print(f'Epoch: {epoch}, Loss: {avg_train_loss}')\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        accurate = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader, 0):\n",
    "                inputs, labels = data\n",
    "                outputs = model(inputs)\n",
    "                # print(outputs)\n",
    "                # output = torch.argmax(outputs, 1)\n",
    "                output = torch.round(outputs)\n",
    "                accurate += torch.sum(output == labels).item()\n",
    "                \n",
    "                val_loss = criterion(outputs, labels)\n",
    "                total_loss += val_loss.item()\n",
    "        \n",
    "        avg_val_loss = total_loss / len(val_loader)\n",
    "        accuracy = accurate / len(y_val)\n",
    "        \n",
    "        # scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch: {epoch}, Validation Loss:{avg_val_loss}, Accuracy: {accuracy}, accurate: {accurate}, total: {len(y_val)}')\n",
    "        \n",
    "        # Check if validation loss improved\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            patience_counter = 0  # Reset counter if validation loss improved\n",
    "            best_accu_epoch = epoch\n",
    "            torch.save(model.state_dict(), f'model/best_model_epoch{epoch}.pth')  # Save the best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation accuracy did not improve for {patience_counter} epochs.\")\n",
    "        \n",
    "        #  # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs with no improvement. Best accuracy: {best_val_accuracy} at epoch {best_accu_epoch}\")\n",
    "            break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test data: 50.1875%\n"
     ]
    }
   ],
   "source": [
    "#Testing the model\n",
    "from sklearn.svm import SVC\n",
    "m = SVC(gamma=2, C=1, random_state=42)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wigglerng-jNs1t9jR-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
